# encoding=utf-8
import json
import os
import re
from tqdm import tqdm

# å¯¼å…¥ snippets.py ä¸­çš„å‡½æ•°
from snippets import chat_with_ollama


def extract_score_from_response(response):
    """
    ä»LLMå“åº”ä¸­æå–ç›¸ä¼¼åº¦å¾—åˆ†
    """
    response = response.replace(' ', '').replace('ã€€', '')
    # å¤šç§åŒ¹é…æ¨¡å¼
    patterns = [
        r'(?:ç›¸ä¼¼åº¦|ç›¸ä¼¼æ€§)(?:(?:å¾—)?åˆ†ä¸º|æ˜¯|ï¼š)\s*(\d{1,3})',  # æ¨¡å¼1
        r'(\d{1,3})\s*åˆ†',                                # æ¨¡å¼2
        r'(?:ç›¸ä¼¼åº¦|å¾—åˆ†)[^\d]*(\d{1,3})',                 # æ¨¡å¼3
        r'(\d{1,3})'                                     # æ¨¡å¼4
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, response)
        if matches:
            # è¿‡æ»¤å‡º0-100èŒƒå›´å†…çš„æ•°å­—
            valid_scores = [int(score) for score in matches if score.isdigit() and 0 <= int(score) <= 100]
            if valid_scores:
                # é€‰æ‹©æœ€åä¸€ä¸ªæœ‰æ•ˆå¾—åˆ†
                score = valid_scores[-1]
                # print(f"æå–åˆ°ç›¸ä¼¼åº¦å¾—åˆ†: {score}")
                return str(score)
    
    print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›¸ä¼¼åº¦å¾—åˆ†")
    return "ERROR"


def calculate_similarity(q, cfact):
    """
    è°ƒç”¨Ollamaæ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
    """
    try:
        prompt = """#èƒŒæ™¯#
        ä½ æ˜¯ä¸€ä½æ™ºæ…§å¸æ³•çš„ç›¸ä¼¼æ¡ˆä¾‹æ£€ç´¢çš„æ³•å¾‹ä¸“å®¶ã€‚
        #éœ€æ±‚#
        1.ä½ éœ€è¦åˆ¤æ–­#è¾“å…¥#ä¸­çš„å€™é€‰æ¡ˆä»¶ä¸æŸ¥è¯¢æ¡ˆä»¶çš„ç›¸ä¼¼æ€§ï¼Œä¸¤è€…å‡ç”¨UMLç¬¦å·éš”å¼€ã€‚
        2.è¯·ä»æ¡ˆæƒ…æè¿°ã€å…³é”®çŠ¯ç½ªè¦ç´ ç­‰æ–¹é¢åˆ¤æ–­å€™é€‰æ¡ˆä»¶ä¸æŸ¥è¯¢æ¡ˆä»¶çš„ç›¸ä¼¼æ€§å’Œç›¸å…³æ€§ï¼Œç›¸ä¼¼åº¦å¾—åˆ†åœ¨ 0 åˆ° 100 ä¹‹é—´ã€‚
        3.è¯·æ³¨æ„ï¼Œä½ åªéœ€è¦ç»™å‡ºæœ€ç»ˆçš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå³ä¸€ä¸ª0-100ä¹‹é—´ç²¾ç¡®åˆ°ä¸ªä½æ•°çš„æ•°å­—ï¼Œä¸è¦ç»™å‡º0å’Œ100è¿™æ ·çš„æç«¯æ•°å­—ã€‚
        #è¾“å…¥#
        <å€™é€‰æ¡ˆä»¶>
        {}
        </å€™é€‰æ¡ˆä»¶>
        <æŸ¥è¯¢æ¡ˆä»¶>
        {}
        </æŸ¥è¯¢æ¡ˆä»¶>
        """       
        prompt = prompt.format(cfact[:3500], q[:3500])
        # ä½¿ç”¨Ollamaæ¨¡å‹
        response = chat_with_ollama(model='mrhua/llama3-8b-chinese-lora-law_f16_q4_0', question=prompt)
        print(response)
        # ä½¿ç”¨æå–å‡½æ•°
        score_str = extract_score_from_response(response)
        
        if score_str != "ERROR":
            # å°†0-100çš„å¾—åˆ†è½¬æ¢ä¸º0-1çš„å¾—åˆ†
            score = float(score_str) / 100.0
            print("æå–åˆ°çš„ç›¸å…³æ€§å¾—åˆ†æ˜¯:", score)
        else:
            score = 0.0
            print("æå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¾—åˆ†: 0.0")
        
        return score
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {e}")
        return 0.0


def process_cases(file_path, output_path):
    """
    å¤„ç†æ¡ˆä»¶æ•°æ®å¹¶ç”Ÿæˆç›¸ä¼¼åº¦æ’åºç»“æœ
    """
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        querys = f.readlines()
        cases = []
        for query in tqdm(querys[:]):
            cases.append(eval(query))

    # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(output_path):
        os.remove(output_path)
    
    # é€ä¸ªå¤„ç†æ¯ä¸ªæ¡ˆä¾‹
    for case in tqdm(cases, desc="Processing cases"):
        qid = case['qid']
        cid = case['cid']
        q = case['q']
        cfact = case['cfact']
        label = case['label']

        # è®¡ç®—ç›¸ä¼¼åº¦
        sim = calculate_similarity(q, cfact)

        # æ„å»ºç»“æœå¯¹è±¡
        result = {"qid": qid, "cid": cid, "sim": sim, "label": label}

        # æŒ‰è¡Œå†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')


if __name__ == "__main__":
    # å®šä¹‰æ‰€æœ‰æ•°æ®é›†çš„é…ç½®ï¼Œä½¿ç”¨æ›´æ–°åçš„è·¯å¾„
    datasets_config = {
        # 'ELAM': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/keyfactor/elam_base_train.json',
        #     'output': 'elam.json'
        # },
        # 'ELAM-keyfact': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/keyfactor/elam_remove_keyfact.json',
        #     'output': 'elam_remove_keyfact.json'
        # },
        # 'ELAM-keyfactor': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/keyfactor/elam_remove_keyfactor.json',
        #     'output': 'elam_remove_keyfactor.json'
        # },
        # 'CAIL': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/fairness/cail_test.json',
        #     'output': 'cail_test.json'
        # },
        # 'fairness': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/fairness/fair_test.json',
        #     'output': 'fairness.json'
        # },
        # 'fair-name': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/fairness/cail_name.json',
        #     'output': 'cail_name.json'
        # },
        # 'fair-sex': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/fairness/cail_sex.json',
        #     'output': 'cail_sex.json'
        # },
        # 'fair-race': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/fairness/cail_race.json',
        #     'output': 'cail_race.json'
        # },
        # 'LeCaRD': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/base/lecard_common_test.json',
        #     'output': 'lecard_common.json'
        # },
        # 'direct_delete': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/incomplete/direct_delete_test.json',
        #     'output': 'direct_delete.json'
        # },
        # 'fact-abstract': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/incomplete/fact_abstract_test.json',
        #     'output': 'fact_abstract.json'
        # },
        # 'baihua': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/baihua/baihua_test.json',
        #     'output': 'baihua.json'
        # },
        # 'judgment': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/Judgment/one_people.json',
        #     'output': 'judgment.json'
        # },
        # 'conf-bj': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/confused/trainDataset/bj.json',
        #     'output': 'conf_bj.json'
        # },
        # 'conf-dq': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/confused/trainDataset/dq.json',
        #     'output': 'conf_dq.json'
        # },
        # 'conf-gysr': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/confused/trainDataset/gysr.json',
        #     'output': 'conf_gysr.json'
        # },
        # 'conf-jtzs': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/confused/trainDataset/jtzs.json',
        #     'output': 'conf_jtzs.json'
        # },
        # 'conf-tw': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/confused/trainDataset/tw.json',
        #     'output': 'conf_tw.json'
        # }
        # 'multi': {
        #     'input': '/home/hz/yangyi/LCRcheck/data/cases2/MultiDefendant/multi_defendant.json',
        #     'output': 'multi_defendant.json'
        # },
        'multi_final': {
            'input': '/home/hz/yangyi/LCRcheck/data/cases2/MultiDefendant/multi_defendant_final.json',
            'output': 'multi_defendant_final.json'
        }
    }
    
    # è¾“å‡ºæ–‡ä»¶å¤¹åŸºç¡€è·¯å¾„ï¼Œä½¿ç”¨æ–°çš„è·¯å¾„
    output_base_path = "/home/hz/yangyi/LCRcheck/results/test/llama3_law"
    
    # å¾ªç¯å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset_name, config in datasets_config.items():
        input_file = config['input']
        output_filename = config['output']
        
        print(f"\nå¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_file):
            print(f"è­¦å‘Š: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®é›† {dataset_name}: {input_file}")
            continue
        
        # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(output_base_path, output_filename)
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        try:
            # å¤„ç†æ¡ˆä»¶æ•°æ®
            process_cases(input_file, output_file)
            print(f"âœ… æ•°æ®é›† {dataset_name} å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}")
            continue
    
    print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")